{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e588eade-1a0b-4475-b64f-e34872402893",
   "metadata": {},
   "source": [
    "# Label cell types using CellTypist Models\n",
    "\n",
    "To build our reference, we would like to start with labels that originate from published cell type references. \n",
    "\n",
    "One of the approaches for this cell type labeling is CellTypist, a model-based approach to cell type labeling.  \n",
    "\n",
    "CellTypist is described [on their website](https://www.celltypist.org/), and in this publication:  \n",
    "\n",
    "Dom√≠nguez Conde, C. et al. Cross-tissue immune cell analysis reveals tissue-specific features in humans. Science 376, eabl5197 (2022)\n",
    "\n",
    "Here, we'll load in our cells individually, and assign labels based on the highest resolution of our 3-level annotated PBMC reference:  \n",
    "\n",
    "- AIFI_L3:\n",
    "    - 71 types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a83eb8-37dd-49ad-9670-fff2365d5d13",
   "metadata": {},
   "source": [
    "## Load Packages\n",
    "\n",
    "`anndata`: Data structures for scRNA-seq  \n",
    "`celltypist`: Model-based cell type annotation  \n",
    "`concurrent.futures`: parallelization methods  \n",
    "`datetime`: date and time functions  \n",
    "`h5py`: HDF5 file I/O  \n",
    "`hisepy`: The HISE SDK for Python  \n",
    "`numpy`: Mathematical data structures and computation  \n",
    "`os`: operating system calls  \n",
    "`pandas`: DataFrame data structures  \n",
    "`re`: Regular expressions  \n",
    "`scanpy`: scRNA-seq analysis  \n",
    "`scipy.sparse`: Spare matrix data structures  \n",
    "`shutil`: Shell utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf3c687-3777-495f-8a9a-39e1e65fd449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import anndata\n",
    "import celltypist\n",
    "from celltypist import models\n",
    "import concurrent.futures\n",
    "from datetime import date\n",
    "import h5py\n",
    "import hisepy\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "import re\n",
    "import scanpy as sc\n",
    "import scipy.sparse as scs\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a1635d-463f-482a-9bdc-fec32f8b58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = 'BR1'\n",
    "subject_sex = 'Female'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5ec0a-fb76-4a90-b5bd-412676fbcf6c",
   "metadata": {},
   "source": [
    "Load a model to prevent CellTypist from loading all models per core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108bd11b-4b0c-426e-8e1e-c263bcc34292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìú Retrieving model list from server https://celltypist.cog.sanger.ac.uk/models/models.json\n",
      "üìö Total models in list: 44\n",
      "üìÇ Storing models in /root/.celltypist/data/models\n",
      "üíæ Total models to download: 1\n",
      "üíæ Downloading model [1/1]: Immune_All_High.pkl\n"
     ]
    }
   ],
   "source": [
    "models.download_models(\n",
    "    force_update = True,\n",
    "    model = ['Immune_All_High.pkl']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4f4cd-b6f7-47fe-9ad8-ff8bfd0b4d63",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "This function allows easy reading of .csv files stored in HISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af491edf-c0cc-420c-b1cd-a409cc90c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_uuid(csv_uuid):\n",
    "    csv_path = '/home/jupyter/cache/{u}'.format(u = csv_uuid)\n",
    "    if not os.path.isdir(csv_path):\n",
    "        hise_res = hisepy.reader.cache_files([csv_uuid])\n",
    "    csv_filename = os.listdir(csv_path)[0]\n",
    "    csv_file = '{p}/{f}'.format(p = csv_path, f = csv_filename)\n",
    "    df = pd.read_csv(csv_file, index_col = 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f030fe-6c83-4165-b9ea-e15ccca2ae5a",
   "metadata": {},
   "source": [
    "This function allows easy identification of the cached file path for files retrieved from HISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c74459-2ef6-4c47-a56f-6346e245aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_path_uuid(file_uuid):\n",
    "    file_path = '/home/jupyter/cache/{u}'.format(u = file_uuid)\n",
    "    if not os.path.isdir(file_path):\n",
    "        hise_res = hisepy.reader.cache_files([file_uuid])\n",
    "    filename = os.listdir(file_path)[0]\n",
    "    full_path = '{p}/{f}'.format(p = file_path, f = filename)\n",
    "    return full_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f531e5-2540-4cee-98c4-2fa7eb632c75",
   "metadata": {},
   "source": [
    "These functions will retrieve data for a sample, assemble an AnnData object, perform normalization and log transformation, then generate predictions for each of the 3 models retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a81ae7c-7930-4b56-a3b2-0f6cbfe711c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a function to read count data\n",
    "def read_mat(h5_con):\n",
    "    mat = scs.csc_matrix(\n",
    "        (h5_con['matrix']['data'][:], # Count values\n",
    "         h5_con['matrix']['indices'][:], # Row indices\n",
    "         h5_con['matrix']['indptr'][:]), # Pointers for column positions\n",
    "        shape = tuple(h5_con['matrix']['shape'][:]) # Matrix dimensions\n",
    "    )\n",
    "    return mat\n",
    "\n",
    "# define a function to read obeservation metadata (i.e. cell metadata)\n",
    "def read_obs(h5con):\n",
    "    bc = h5con['matrix']['barcodes'][:]\n",
    "    bc = [x.decode('UTF-8') for x in bc]\n",
    "\n",
    "    # Initialized the DataFrame with cell barcodes\n",
    "    obs_df = pd.DataFrame({ 'barcodes' : bc })\n",
    "\n",
    "    # Get the list of available metadata columns\n",
    "    obs_columns = h5con['matrix']['observations'].keys()\n",
    "\n",
    "    # For each column\n",
    "    for col in obs_columns:\n",
    "        # Read the values\n",
    "        values = h5con['matrix']['observations'][col][:]\n",
    "        # Check for byte storage\n",
    "        if(isinstance(values[0], (bytes, bytearray))):\n",
    "            # Decode byte strings\n",
    "            values = [x.decode('UTF-8') for x in values]\n",
    "        # Add column to the DataFrame\n",
    "        obs_df[col] = values\n",
    "\n",
    "    obs_df = obs_df.set_index('barcodes', drop = False)\n",
    "    \n",
    "    return obs_df\n",
    "\n",
    "# define a function to construct anndata object from a h5 file\n",
    "def read_h5_anndata(h5_con):\n",
    "    #h5_con = h5py.File(h5_file, mode = 'r')\n",
    "    # extract the expression matrix\n",
    "    mat = read_mat(h5_con)\n",
    "    # extract gene names\n",
    "    genes = h5_con['matrix']['features']['name'][:]\n",
    "    genes = [x.decode('UTF-8') for x in genes]\n",
    "    # extract metadata\n",
    "    obs_df = read_obs(h5_con)\n",
    "    # construct anndata\n",
    "    adata = anndata.AnnData(mat.T,\n",
    "                             obs = obs_df)\n",
    "    # make sure the gene names aligned\n",
    "    adata.var_names = genes\n",
    "\n",
    "    adata.var_names_make_unique()\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bbb3440-41ca-4942-850d-1ad053da164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adata(uuid):\n",
    "    # Load the file using HISE\n",
    "    res = hisepy.reader.read_files([uuid])\n",
    "\n",
    "    # If there's an error, read_files returns a list instead of a dictionary.\n",
    "    # We should raise and exception with the message when this happens.\n",
    "    if(isinstance(res, list)):\n",
    "        error_message = res[0]['message']\n",
    "        raise Exception('{u}: {e}'.format(u = uuid, e = error_message))\n",
    "    \n",
    "    # Read the file to adata\n",
    "    h5_con = res['values'][0]\n",
    "    adata = read_h5_anndata(h5_con)\n",
    "    \n",
    "    # Close the file now that we're done with it\n",
    "    h5_con.close()\n",
    "\n",
    "    return(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bcd92d2-af9e-4e6f-8f51-450796d40476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(adata, model, model_name, out_dir = \"output\"):\n",
    "    # Make output directories\n",
    "    model_dir = \"{d}/{m}\".format(d = out_dir, m = model_name)\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    sample_id = adata.obs['pbmc_sample_id'].unique()[0]\n",
    "    label_file = \"{d}/{s}_{m}_labels.csv\".format(d = model_dir, s = sample_id, m = model_name)\n",
    "\n",
    "    if os.path.exists(label_file):\n",
    "        print(\"{s}: {m} Previously computed; Skipping.\".format(s = sample_id, m = model_name))\n",
    "    else:\n",
    "        # Perform prediction\n",
    "        predictions = celltypist.annotate(\n",
    "            adata, \n",
    "            model = model, \n",
    "            majority_voting = True)\n",
    "    \n",
    "        # Write output\n",
    "        \n",
    "        prob_file = \"{d}/{s}_{m}_probability_mat.parquet\".format(d = model_dir, s = sample_id, m = model_name)\n",
    "        prob = predictions.probability_matrix\n",
    "        prob.to_parquet(prob_file)\n",
    "    \n",
    "        dec_file = \"{d}/{s}_{m}_decision_mat.parquet\".format(d = model_dir, s = sample_id, m = model_name)\n",
    "        predictions.decision_matrix.to_parquet(dec_file)\n",
    "        \n",
    "        labels = predictions.predicted_labels\n",
    "        labels = labels.rename({'predicted_labels': model_name}, axis = 1)\n",
    "        \n",
    "        prob_scores = []\n",
    "        for i in range(labels.shape[0]):\n",
    "            prob_scores.append(prob.loc[labels.index.to_list()[i],labels[model_name][i]])\n",
    "        labels['{m}_score'.format(m = model_name)] = prob_scores\n",
    "        labels.to_csv(label_file)\n",
    "    \n",
    "def process_data(file_uuid, sample_id):\n",
    "    out_dir = \"output\"\n",
    "    check_file = '{d}/{m}/{s}_{m}_labels.csv'.format(d = out_dir, m = 'AIFI_L3', s = sample_id)\n",
    "\n",
    "    if os.path.exists(check_file):\n",
    "        print('{s} Previously labeled; Skipping.'.format(s = sample_id))\n",
    "    else:\n",
    "        # Load cells from HISE .h5 files\n",
    "        adata = get_adata(file_uuid)\n",
    "        \n",
    "        # Normalize data\n",
    "        sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "        adata.obs.index = adata.obs['barcodes']\n",
    "        \n",
    "        # Predict cell types\n",
    "        for model_name,model_path in model_paths.items():\n",
    "            run_prediction(\n",
    "                adata,\n",
    "                model_path,\n",
    "                model_name,\n",
    "                out_dir\n",
    "            )\n",
    "        \n",
    "        del adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97c30414-e0ce-4746-9e3f-b52f2e6db98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_id(n = 3):\n",
    "    import periodictable\n",
    "    from random import randrange\n",
    "    rand_el = []\n",
    "    for i in range(n):\n",
    "        el = randrange(0,118)\n",
    "        rand_el.append(periodictable.elements[el].name)\n",
    "    rand_str = '-'.join(rand_el)\n",
    "    return rand_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd8976-9454-49e0-932f-e8a13dd8c768",
   "metadata": {},
   "source": [
    "## Obtain CellTypist Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49791ed9-fe42-44bd-bc04-e395ae5db538",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uuids = {\n",
    "    'AIFI_L3': '671d1e43-bd32-4fea-bdda-d19a0484e664',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "824be9f5-c36c-425b-b439-4f8419c3e043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading fileID: 671d1e43-bd32-4fea-bdda-d19a0484e664\n",
      "Files have been successfully downloaded!\n"
     ]
    }
   ],
   "source": [
    "model_paths = {}\n",
    "for name,uuid in model_uuids.items():\n",
    "    model_paths[name] = read_path_uuid(uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38abd1d6-df98-4e6a-beba-cc06c55e52cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AIFI_L3': '/home/jupyter/cache/671d1e43-bd32-4fea-bdda-d19a0484e664/ref_pbmc_clean_celltypist_model_AIFI_L3_2024-04-19.pkl'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a4a141-780e-4eab-b9c1-d7b5865dfabd",
   "metadata": {},
   "source": [
    "## Read sample metadata from HISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f55a5a2-a5e0-42ce-a34c-9add7cbef948",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_meta_file_uuid = 'd82c5c42-ae5f-4e67-956e-cd3b7bf88105'\n",
    "file_query = hisepy.reader.read_files(\n",
    "    [sample_meta_file_uuid]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21ce05a3-f4dc-4475-923a-f7049b833326",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = file_query['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58473e89-5816-4b27-a3db-4b0fc3dae1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(868, 33)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2aa9e-c8ff-45de-aff8-d025ee1dfef8",
   "metadata": {},
   "source": [
    "### Filter metadata for selected cohort and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6afd4c07-1c1e-488c-8663-6c25b39e08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = meta_data[meta_data['cohort.cohortGuid'] == cohort]\n",
    "meta_data = meta_data[meta_data['subject.biologicalSex'] == subject_sex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b98a13c-7d7e-4e64-a779-033419284ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238, 33)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79d0ec-ec3a-4644-a149-a3058255904f",
   "metadata": {},
   "source": [
    "## Apply across files\n",
    "\n",
    "Here, we'll use `concurrent.futures` to apply the function above to our files in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e28d2e7-a974-4721-8d86-ff79605aa155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_dir = 'output'\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4932e96-7872-40a8-8ef5-f3fd81367394",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_uuids = meta_data['file.id'].to_list()\n",
    "sample_ids = meta_data['pbmc_sample_id'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06510b9e-62f9-4fd9-812c-5f94603b3b29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PB00001-01 Previously labeled; Skipping.PB00007-01 Previously labeled; Skipping.PB00008-01 Previously labeled; Skipping.PB00006-01 Previously labeled; Skipping.PB00013-01 Previously labeled; Skipping.PB00012-01 Previously labeled; Skipping.PB00022-01 Previously labeled; Skipping.PB00019-01 Previously labeled; Skipping.PB00024-01 Previously labeled; Skipping.PB00023-05 Previously labeled; Skipping.PB00031-05 Previously labeled; Skipping.PB00027-05 Previously labeled; Skipping.PB00030-02 Previously labeled; Skipping.PB00032-05 Previously labeled; Skipping.\n",
      "PB00033-06 Previously labeled; Skipping.PB00039-03 Previously labeled; Skipping.\n",
      "PB00035-01 Previously labeled; Skipping.PB00037-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "PB00040-01 Previously labeled; Skipping.\n",
      "PB00042-01 Previously labeled; Skipping.PB00046-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB00048-01 Previously labeled; Skipping.PB00050-01 Previously labeled; Skipping.\n",
      "\n",
      "PB00141-01 Previously labeled; Skipping.PB00047-01 Previously labeled; Skipping.\n",
      "PB00003-01 Previously labeled; Skipping.\n",
      "\n",
      "PB00147-01 Previously labeled; Skipping.PB00144-01 Previously labeled; Skipping.PB00153-01 Previously labeled; Skipping.PB00142-01 Previously labeled; Skipping.PB00152-01 Previously labeled; Skipping.PB00154-01 Previously labeled; Skipping.PB00155-01 Previously labeled; Skipping.PB00164-01 Previously labeled; Skipping.\n",
      "PB00162-01 Previously labeled; Skipping.PB00150-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB00253-01 Previously labeled; Skipping.PB00161-01 Previously labeled; Skipping.PB00169-03 Previously labeled; Skipping.PB00256-01 Previously labeled; Skipping.\n",
      "PB00286-05 Previously labeled; Skipping.PB00257-01 Previously labeled; Skipping.\n",
      "PB00259-01 Previously labeled; Skipping.PB00262-01 Previously labeled; Skipping.PB00298-02 Previously labeled; Skipping.PB00166-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB00267-01 Previously labeled; Skipping.PB00263-03 Previously labeled; Skipping.PB00268-01 Previously labeled; Skipping.PB00327-01 Previously labeled; Skipping.PB00325-01 Previously labeled; Skipping.PB00270-01 Previously labeled; Skipping.PB00272-01 Previously labeled; Skipping.PB00271-03 Previously labeled; Skipping.PB00276-03 Previously labeled; Skipping.\n",
      "\n",
      "PB00281-01 Previously labeled; Skipping.PB00278-05 Previously labeled; Skipping.\n",
      "\n",
      "PB00282-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "PB00283-01 Previously labeled; Skipping.PB00290-01 Previously labeled; Skipping.\n",
      "PB00284-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB00332-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB00341-01 Previously labeled; Skipping.PB00345-01 Previously labeled; Skipping.\n",
      "PB00349-01 Previously labeled; Skipping.PB00347-01 Previously labeled; Skipping.PB00350-01 Previously labeled; Skipping.PB00354-01 Previously labeled; Skipping.\n",
      "PB00355-01 Previously labeled; Skipping.\n",
      "\n",
      "PB00362-01 Previously labeled; Skipping.PB00366-01 Previously labeled; Skipping.PB00365-01 Previously labeled; Skipping.\n",
      "\n",
      "PB00367-01 Previously labeled; Skipping.PB00370-05 Previously labeled; Skipping.PB00372-01 Previously labeled; Skipping.PB00373-01 Previously labeled; Skipping.PB00385-01 Previously labeled; Skipping.PB00376-01 Previously labeled; Skipping.\n",
      "PB00392-01 Previously labeled; Skipping.PB00380-01 Previously labeled; Skipping.PB00502-01 Previously labeled; Skipping.PB00507-01 Previously labeled; Skipping.\n",
      "PB00338-01 Previously labeled; Skipping.PB00331-01 Previously labeled; Skipping.PB00524-01 Previously labeled; Skipping.PB00525-01 Previously labeled; Skipping.\n",
      "\n",
      "PB00528-01 Previously labeled; Skipping.PB00529-01 Previously labeled; Skipping.PB00532-01 Previously labeled; Skipping.PB00544-01 Previously labeled; Skipping.\n",
      "PB00547-01 Previously labeled; Skipping.\n",
      "PB00546-01 Previously labeled; Skipping.PB00555-01 Previously labeled; Skipping.\n",
      "PB00575-01 Previously labeled; Skipping.PB00576-01 Previously labeled; Skipping.PB00586-01 Previously labeled; Skipping.PB00585-01 Previously labeled; Skipping.PB00589-01 Previously labeled; Skipping.\n",
      "PB00587-01 Previously labeled; Skipping.PB00591-01 Previously labeled; Skipping.PB00601-01 Previously labeled; Skipping.PB00595-01 Previously labeled; Skipping.PB00594-01 Previously labeled; Skipping.\n",
      "PB00610-02 Previously labeled; Skipping.\n",
      "PB00612-01 Previously labeled; Skipping.PB00616-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB00617-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB00622-01 Previously labeled; Skipping.PB00621-01 Previously labeled; Skipping.PB00628-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB00630-01 Previously labeled; Skipping.\n",
      "\n",
      "PB00631-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "PB00633-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB00634-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB00635-01 Previously labeled; Skipping.PB00643-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB00645-01 Previously labeled; Skipping.\n",
      "PB00646-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "PB01411-01 Previously labeled; Skipping.PB00650-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB01425-01 Previously labeled; Skipping.PB01430-01 Previously labeled; Skipping.\n",
      "\n",
      "PB01434-01 Previously labeled; Skipping.PB01435-01 Previously labeled; Skipping.\n",
      "PB01440-01 Previously labeled; Skipping.PB01450-01 Previously labeled; Skipping.PB01457-01 Previously labeled; Skipping.PB01454-01 Previously labeled; Skipping.PB01459-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB01514-01 Previously labeled; Skipping.PB01519-01 Previously labeled; Skipping.PB01520-01 Previously labeled; Skipping.\n",
      "PB01524-01 Previously labeled; Skipping.PB01525-01 Previously labeled; Skipping.\n",
      "PB01531-01 Previously labeled; Skipping.PB01548-01 Previously labeled; Skipping.PB01552-01 Previously labeled; Skipping.PB01559-01 Previously labeled; Skipping.\n",
      "PB01565-01 Previously labeled; Skipping.PB01574-01 Previously labeled; Skipping.PB01573-01 Previously labeled; Skipping.PB01583-01 Previously labeled; Skipping.\n",
      "PB01587-01 Previously labeled; Skipping.PB01596-01 Previously labeled; Skipping.PB01602-01 Previously labeled; Skipping.\n",
      "PB01603-01 Previously labeled; Skipping.\n",
      "PB01606-01 Previously labeled; Skipping.PB01972-01 Previously labeled; Skipping.PB01975-01 Previously labeled; Skipping.\n",
      "PB02012-01 Previously labeled; Skipping.PB02233-01 Previously labeled; Skipping.\n",
      "PB02242-02 Previously labeled; Skipping.PB02238-01 Previously labeled; Skipping.PB02244-01 Previously labeled; Skipping.\n",
      "PB02247-01 Previously labeled; Skipping.PB02248-01 Previously labeled; Skipping.\n",
      "PB02253-01 Previously labeled; Skipping.PB02257-01 Previously labeled; Skipping.PB02264-01 Previously labeled; Skipping.PB02259-01 Previously labeled; Skipping.PB02263-02 Previously labeled; Skipping.PB02266-01 Previously labeled; Skipping.\n",
      "\n",
      "PB02267-01 Previously labeled; Skipping.PB02270-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB02271-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "PB02274-01 Previously labeled; Skipping.PB02275-01 Previously labeled; Skipping.\n",
      "PB02276-02 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB02311-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB02312-001 Previously labeled; Skipping.\n",
      "\n",
      "PB02333-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "PB02336-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB02338-001 Previously labeled; Skipping.\n",
      "PB02343-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB02352-001 Previously labeled; Skipping.\n",
      "\n",
      "PB02444-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "PB02446-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB02449-001 Previously labeled; Skipping.\n",
      "\n",
      "PB02465-001 Previously labeled; Skipping.PB02462-001 Previously labeled; Skipping.\n",
      "\n",
      "PB02466-001 Previously labeled; Skipping.PB02468-001 Previously labeled; Skipping.\n",
      "PB02470-001 Previously labeled; Skipping.PB02471-001 Previously labeled; Skipping.PB02475-001 Previously labeled; Skipping.\n",
      "PB02480-001 Previously labeled; Skipping.PB02476-001 Previously labeled; Skipping.\n",
      "PB02483-001 Previously labeled; Skipping.\n",
      "\n",
      "PB02486-001 Previously labeled; Skipping.PB02490-001 Previously labeled; Skipping.PB02487-001 Previously labeled; Skipping.PB02494-001 Previously labeled; Skipping.PB02519-001 Previously labeled; Skipping.\n",
      "PB02496-001 Previously labeled; Skipping.PB02520-001 Previously labeled; Skipping.\n",
      "PB02527-002 Previously labeled; Skipping.PB02529-001 Previously labeled; Skipping.\n",
      "PB02541-001 Previously labeled; Skipping.PB02550-001 Previously labeled; Skipping.\n",
      "PB02552-001 Previously labeled; Skipping.PB02553-002 Previously labeled; Skipping.PB02556-002 Previously labeled; Skipping.\n",
      "PB02557-002 Previously labeled; Skipping.PB02561-001 Previously labeled; Skipping.\n",
      "PB02562-001 Previously labeled; Skipping.PB02559-002 Previously labeled; Skipping.PB02564-001 Previously labeled; Skipping.PB02571-001 Previously labeled; Skipping.\n",
      "PB02572-001 Previously labeled; Skipping.PB02577-001 Previously labeled; Skipping.\n",
      "PB02570-002 Previously labeled; Skipping.PB02581-001 Previously labeled; Skipping.PB02578-001 Previously labeled; Skipping.PB02589-001 Previously labeled; Skipping.PB02590-001 Previously labeled; Skipping.PB02591-001 Previously labeled; Skipping.PB02592-002 Previously labeled; Skipping.\n",
      "PB03062-002 Previously labeled; Skipping.PB03068-002 Previously labeled; Skipping.\n",
      "\n",
      "PB03074-001 Previously labeled; Skipping.PB03083-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB03091-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "PB03096-001 Previously labeled; Skipping.PB03101-001 Previously labeled; Skipping.\n",
      "PB03105-001 Previously labeled; Skipping.\n",
      "PB03107-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB03896-001 Previously labeled; Skipping.PB03904-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "PB03915-001 Previously labeled; Skipping.PB03920-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB03922-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB03923-001 Previously labeled; Skipping.PB03925-001 Previously labeled; Skipping.\n",
      "PB03926-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB03928-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB00299-02 Previously labeled; Skipping.\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üî¨ Input data has 20586 cells and 33538 genes\n",
      "üîó Matching reference genes in the model\n",
      "üî¨ Input data has 18405 cells and 33538 genes\n",
      "üîó Matching reference genes in the model\n",
      "üß¨ 2504 features used for prediction\n",
      "‚öñÔ∏è Scaling input data\n",
      "üß¨ 2504 features used for prediction\n",
      "‚öñÔ∏è Scaling input data\n",
      "üñãÔ∏è Predicting labels\n",
      "‚úÖ Prediction done!\n",
      "üëÄ Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "üñãÔ∏è Predicting labels\n",
      "‚úÖ Prediction done!\n",
      "üëÄ Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "‚õìÔ∏è Over-clustering input data with resolution set to 10\n",
      "‚õìÔ∏è Over-clustering input data with resolution set to 15\n",
      "üó≥Ô∏è Majority voting the predictions\n",
      "‚úÖ Majority voting done!\n",
      "üó≥Ô∏è Majority voting the predictions\n",
      "‚úÖ Majority voting done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Process each subset in parallel\n",
    "pool_executor = concurrent.futures.ProcessPoolExecutor(max_workers = 60)\n",
    "with pool_executor as executor:\n",
    "    \n",
    "    futures = []\n",
    "    for i in range(len(file_uuids)):\n",
    "        file_uuid = file_uuids[i]\n",
    "        sample_id = sample_ids[i]\n",
    "        futures.append(executor.submit(process_data, file_uuid, sample_id))\n",
    "\n",
    "    # Check for errors when parallel processes return results\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            print(future.result())\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74db89fd-2f92-4148-9a6d-f115c1efe52e",
   "metadata": {},
   "source": [
    "## Assemble results\n",
    "\n",
    "For each model, we'll assemble the results as a .csv file that we can utilize later for subclustering and analysis of major cell classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa2c2832-1f92-4830-95c1-b87d44c6e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(model_paths.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "942b3d87-2fbb-461d-8f04-89c13118d01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n"
     ]
    }
   ],
   "source": [
    "out_files = []\n",
    "for model in models:\n",
    "    model_path = 'output/{m}'.format(m = model)\n",
    "    model_path_files = os.listdir(model_path)\n",
    "    model_files = []\n",
    "    for model_path_file in model_path_files:\n",
    "        if 'labels' in model_path_file:\n",
    "            model_files.append(model_path_file)\n",
    "    print(len(model_files))\n",
    "    \n",
    "    model_list = []\n",
    "    for model_file in model_files:\n",
    "        df = pd.read_csv('output/{m}/{f}'.format(m = model, f = model_file))\n",
    "        model_list.append(df)\n",
    "    model_df = pd.concat(model_list)\n",
    "\n",
    "    out_csv = 'output/diha_celltypist_{c}_{s}_{m}_{d}.csv'.format(\n",
    "        c = cohort, s = subject_sex, m = model, d = date.today())\n",
    "    out_files.append(out_csv)\n",
    "    \n",
    "    model_df.to_csv(out_csv)\n",
    "\n",
    "    out_parquet = 'output/diha_celltypist_{c}_{s}_{m}_{d}.parquet'.format(\n",
    "        c = cohort, s = subject_sex, m = model, d = date.today())\n",
    "    out_files.append(out_parquet)\n",
    "    \n",
    "    model_df.to_parquet(out_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd746b1-8066-44a5-8613-f401e6fce07e",
   "metadata": {},
   "source": [
    "## Upload assembled data to HISE\n",
    "\n",
    "Finally, we'll use `hisepy.upload.upload_files()` to send a copy of our output to HISE to use for downstream analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66e3e599-872c-4677-8bee-08d04003958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_space_uuid = 'de025812-5e73-4b3c-9c3b-6d0eac412f2a'\n",
    "title = 'DIHA PBMC CellTypist L3 {c} {s} {d}'.format(\n",
    "    c = cohort, s = subject_sex, d = date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "afa5544b-f81d-41bc-8fb6-a976b075f7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'niobium-neon-gadolinium'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_id = element_id()\n",
    "search_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4bffe38f-4223-4919-b46a-a4b8536dde06",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_files = list(model_uuids.values()) + [sample_meta_file_uuid] + meta_data['file.id'].to_list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce9877bf-cc8a-4b92-8b9e-ba6f4964f79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['671d1e43-bd32-4fea-bdda-d19a0484e664',\n",
       " 'd82c5c42-ae5f-4e67-956e-cd3b7bf88105',\n",
       " 'fec489f9-9a74-4635-aa91-d2bf09d1faec',\n",
       " '40efd03a-cb2f-4677-af42-a056cbfe5a17',\n",
       " 'ea8d98e9-e99e-4dc6-9e78-9866e0deac68',\n",
       " '1faf2b5f-66e4-4787-8a8b-487621fc4c08',\n",
       " 'cda87fcc-a50e-4c0f-b26c-482a6a88ef41',\n",
       " '7a99c4c8-5438-430a-a37a-5b5f4052c064',\n",
       " 'cd86a3b7-4955-4d76-9b2c-f076024a04eb',\n",
       " '4980030b-919e-4ca6-87fb-29b9163d393c']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_files[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "544fb074-02d5-4ca4-968a-780710e5f48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/diha_celltypist_BR1_Female_AIFI_L3_2024-04-19.csv',\n",
       " 'output/diha_celltypist_BR1_Female_AIFI_L3_2024-04-19.parquet']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80a524b6-7c4c-48c1-8b8c-d05494737bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot determine the current notebook.\n",
      "1) /home/jupyter/IH-A-Aging-Analysis-Notebooks/scrna-seq_analysis/02-reference_labeling/08a-Python_label_predictions_celltypist_L3_BR1_Female.ipynb\n",
      "2) /home/jupyter/IH-A-Aging-Analysis-Notebooks/scrna-seq_analysis/02-reference_labeling/06e-Python_partition_small_cell_classes.ipynb\n",
      "3) /home/jupyter/IH-A-Aging-Analysis-Notebooks/scrna-seq_analysis/02-reference_labeling/11e-Python_review_L3_myeloid_data.ipynb\n",
      "Please select (1-3) \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are trying to upload file_ids... ['output/diha_celltypist_BR1_Female_AIFI_L3_2024-04-19.csv', 'output/diha_celltypist_BR1_Female_AIFI_L3_2024-04-19.parquet']. Do you truly want to proceed?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(y/n) y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'trace_id': '7a0e7f9f-3120-475f-a0f2-d3dd282dfadf',\n",
       " 'files': ['output/diha_celltypist_BR1_Female_AIFI_L3_2024-04-19.csv',\n",
       "  'output/diha_celltypist_BR1_Female_AIFI_L3_2024-04-19.parquet']}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hisepy.upload.upload_files(\n",
    "    files = out_files,\n",
    "    study_space_id = study_space_uuid,\n",
    "    title = title,\n",
    "    input_file_ids = in_files,\n",
    "    destination = search_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1df57b01-3914-48ae-9cc2-f03faec7473c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "anndata             0.10.3\n",
       "celltypist          1.6.2\n",
       "h5py                3.10.0\n",
       "hisepy              0.3.0\n",
       "numpy               1.25.2\n",
       "pandas              2.1.4\n",
       "scanpy              1.9.6\n",
       "scipy               1.11.4\n",
       "session_info        1.0.0\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "PIL                         10.0.1\n",
       "anyio                       NA\n",
       "arrow                       1.3.0\n",
       "asttokens                   NA\n",
       "attr                        23.2.0\n",
       "attrs                       23.2.0\n",
       "babel                       2.14.0\n",
       "beatrix_jupyterlab          NA\n",
       "brotli                      NA\n",
       "cachetools                  5.3.1\n",
       "certifi                     2024.02.02\n",
       "cffi                        1.16.0\n",
       "charset_normalizer          3.3.2\n",
       "cloudpickle                 2.2.1\n",
       "colorama                    0.4.6\n",
       "comm                        0.1.4\n",
       "cryptography                41.0.7\n",
       "cycler                      0.10.0\n",
       "cython_runtime              NA\n",
       "dateutil                    2.8.2\n",
       "db_dtypes                   1.1.1\n",
       "debugpy                     1.8.0\n",
       "decorator                   5.1.1\n",
       "defusedxml                  0.7.1\n",
       "deprecated                  1.2.14\n",
       "exceptiongroup              1.2.0\n",
       "executing                   2.0.1\n",
       "fastjsonschema              NA\n",
       "fqdn                        NA\n",
       "google                      NA\n",
       "greenlet                    2.0.2\n",
       "grpc                        1.58.0\n",
       "grpc_status                 NA\n",
       "idna                        3.6\n",
       "igraph                      0.10.8\n",
       "importlib_metadata          NA\n",
       "ipykernel                   6.28.0\n",
       "ipython_genutils            0.2.0\n",
       "ipywidgets                  8.1.1\n",
       "isoduration                 NA\n",
       "jedi                        0.19.1\n",
       "jinja2                      3.1.2\n",
       "joblib                      1.3.2\n",
       "json5                       NA\n",
       "jsonpointer                 2.4\n",
       "jsonschema                  4.20.0\n",
       "jsonschema_specifications   NA\n",
       "jupyter_events              0.9.0\n",
       "jupyter_server              2.12.1\n",
       "jupyterlab_server           2.25.2\n",
       "jwt                         2.8.0\n",
       "kiwisolver                  1.4.5\n",
       "leidenalg                   0.10.1\n",
       "llvmlite                    0.41.0\n",
       "lz4                         4.3.2\n",
       "markupsafe                  2.1.3\n",
       "matplotlib                  3.8.0\n",
       "matplotlib_inline           0.1.6\n",
       "mpl_toolkits                NA\n",
       "mpmath                      1.3.0\n",
       "natsort                     8.4.0\n",
       "nbformat                    5.9.2\n",
       "numba                       0.58.0\n",
       "opentelemetry               NA\n",
       "overrides                   NA\n",
       "packaging                   23.2\n",
       "parso                       0.8.3\n",
       "periodictable               1.5.2\n",
       "pexpect                     4.8.0\n",
       "pickleshare                 0.7.5\n",
       "pkg_resources               NA\n",
       "platformdirs                4.1.0\n",
       "plotly                      5.18.0\n",
       "prettytable                 3.9.0\n",
       "prometheus_client           NA\n",
       "prompt_toolkit              3.0.42\n",
       "proto                       NA\n",
       "psutil                      NA\n",
       "ptyprocess                  0.7.0\n",
       "pure_eval                   0.2.2\n",
       "pyarrow                     13.0.0\n",
       "pydev_ipython               NA\n",
       "pydevconsole                NA\n",
       "pydevd                      2.9.5\n",
       "pydevd_file_utils           NA\n",
       "pydevd_plugins              NA\n",
       "pydevd_tracing              NA\n",
       "pygments                    2.17.2\n",
       "pynvml                      NA\n",
       "pyparsing                   3.1.1\n",
       "pyreadr                     0.5.0\n",
       "pythonjsonlogger            NA\n",
       "pytz                        2023.3.post1\n",
       "referencing                 NA\n",
       "requests                    2.31.0\n",
       "rfc3339_validator           0.1.4\n",
       "rfc3986_validator           0.1.1\n",
       "rpds                        NA\n",
       "send2trash                  NA\n",
       "shapely                     1.8.5.post1\n",
       "six                         1.16.0\n",
       "sklearn                     1.3.2\n",
       "sniffio                     1.3.0\n",
       "socks                       1.7.1\n",
       "sql                         NA\n",
       "sqlalchemy                  2.0.21\n",
       "sqlparse                    0.4.4\n",
       "stack_data                  0.6.2\n",
       "sympy                       1.12\n",
       "termcolor                   NA\n",
       "texttable                   1.7.0\n",
       "threadpoolctl               3.2.0\n",
       "torch                       2.1.2+cu121\n",
       "torchgen                    NA\n",
       "tornado                     6.3.3\n",
       "tqdm                        4.66.1\n",
       "traitlets                   5.9.0\n",
       "typing_extensions           NA\n",
       "uri_template                NA\n",
       "urllib3                     1.26.18\n",
       "wcwidth                     0.2.12\n",
       "webcolors                   1.13\n",
       "websocket                   1.7.0\n",
       "wrapt                       1.15.0\n",
       "xarray                      2023.12.0\n",
       "yaml                        6.0.1\n",
       "zipp                        NA\n",
       "zmq                         25.1.2\n",
       "zoneinfo                    NA\n",
       "zstandard                   0.22.0\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             8.19.0\n",
       "jupyter_client      8.6.0\n",
       "jupyter_core        5.6.1\n",
       "jupyterlab          4.1.5\n",
       "notebook            6.5.4\n",
       "-----\n",
       "Python 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0]\n",
       "Linux-5.15.0-1042-gcp-x86_64-with-glibc2.31\n",
       "-----\n",
       "Session information updated at 2024-04-19 23:27\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import session_info\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a739960e-997b-46f8-9cd4-90bdeb3d672c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
