{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e588eade-1a0b-4475-b64f-e34872402893",
   "metadata": {},
   "source": [
    "# Label cell types using CellTypist Models\n",
    "\n",
    "To build our reference, we would like to start with labels that originate from published cell type references. \n",
    "\n",
    "One of the approaches for this cell type labeling is CellTypist, a model-based approach to cell type labeling.  \n",
    "\n",
    "CellTypist is described [on their website](https://www.celltypist.org/), and in this publication:  \n",
    "\n",
    "Domínguez Conde, C. et al. Cross-tissue immune cell analysis reveals tissue-specific features in humans. Science 376, eabl5197 (2022)\n",
    "\n",
    "Here, we'll load in our cells individually, and assign labels based on the highest resolution of our 3-level annotated PBMC reference:  \n",
    "\n",
    "- AIFI_L3:\n",
    "    - 71 types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a83eb8-37dd-49ad-9670-fff2365d5d13",
   "metadata": {},
   "source": [
    "## Load Packages\n",
    "\n",
    "`anndata`: Data structures for scRNA-seq  \n",
    "`celltypist`: Model-based cell type annotation  \n",
    "`concurrent.futures`: parallelization methods  \n",
    "`datetime`: date and time functions  \n",
    "`h5py`: HDF5 file I/O  \n",
    "`hisepy`: The HISE SDK for Python  \n",
    "`numpy`: Mathematical data structures and computation  \n",
    "`os`: operating system calls  \n",
    "`pandas`: DataFrame data structures  \n",
    "`re`: Regular expressions  \n",
    "`scanpy`: scRNA-seq analysis  \n",
    "`scipy.sparse`: Spare matrix data structures  \n",
    "`shutil`: Shell utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf3c687-3777-495f-8a9a-39e1e65fd449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import anndata\n",
    "import celltypist\n",
    "from celltypist import models\n",
    "import concurrent.futures\n",
    "from datetime import date\n",
    "import h5py\n",
    "import hisepy\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "import re\n",
    "import scanpy as sc\n",
    "import scipy.sparse as scs\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a1635d-463f-482a-9bdc-fec32f8b58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = 'BR1'\n",
    "subject_sex = 'Female'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5ec0a-fb76-4a90-b5bd-412676fbcf6c",
   "metadata": {},
   "source": [
    "Load a model to prevent CellTypist from loading all models per core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108bd11b-4b0c-426e-8e1e-c263bcc34292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📜 Retrieving model list from server https://celltypist.cog.sanger.ac.uk/models/models.json\n",
      "📚 Total models in list: 44\n",
      "📂 Storing models in /root/.celltypist/data/models\n",
      "💾 Total models to download: 1\n",
      "💾 Downloading model [1/1]: Immune_All_High.pkl\n"
     ]
    }
   ],
   "source": [
    "models.download_models(\n",
    "    force_update = True,\n",
    "    model = ['Immune_All_High.pkl']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4f4cd-b6f7-47fe-9ad8-ff8bfd0b4d63",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "This function allows easy reading of .csv files stored in HISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af491edf-c0cc-420c-b1cd-a409cc90c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_uuid(csv_uuid):\n",
    "    csv_path = '/home/jupyter/cache/{u}'.format(u = csv_uuid)\n",
    "    if not os.path.isdir(csv_path):\n",
    "        hise_res = hisepy.reader.cache_files([csv_uuid])\n",
    "    csv_filename = os.listdir(csv_path)[0]\n",
    "    csv_file = '{p}/{f}'.format(p = csv_path, f = csv_filename)\n",
    "    df = pd.read_csv(csv_file, index_col = 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f030fe-6c83-4165-b9ea-e15ccca2ae5a",
   "metadata": {},
   "source": [
    "This function allows easy identification of the cached file path for files retrieved from HISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c74459-2ef6-4c47-a56f-6346e245aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_path_uuid(file_uuid):\n",
    "    file_path = '/home/jupyter/cache/{u}'.format(u = file_uuid)\n",
    "    if not os.path.isdir(file_path):\n",
    "        hise_res = hisepy.reader.cache_files([file_uuid])\n",
    "    filename = os.listdir(file_path)[0]\n",
    "    full_path = '{p}/{f}'.format(p = file_path, f = filename)\n",
    "    return full_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f531e5-2540-4cee-98c4-2fa7eb632c75",
   "metadata": {},
   "source": [
    "These functions will retrieve data for a sample, assemble an AnnData object, perform normalization and log transformation, then generate predictions for each of the 3 models retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a81ae7c-7930-4b56-a3b2-0f6cbfe711c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a function to read count data\n",
    "def read_mat(h5_con):\n",
    "    mat = scs.csc_matrix(\n",
    "        (h5_con['matrix']['data'][:], # Count values\n",
    "         h5_con['matrix']['indices'][:], # Row indices\n",
    "         h5_con['matrix']['indptr'][:]), # Pointers for column positions\n",
    "        shape = tuple(h5_con['matrix']['shape'][:]) # Matrix dimensions\n",
    "    )\n",
    "    return mat\n",
    "\n",
    "# define a function to read obeservation metadata (i.e. cell metadata)\n",
    "def read_obs(h5con):\n",
    "    bc = h5con['matrix']['barcodes'][:]\n",
    "    bc = [x.decode('UTF-8') for x in bc]\n",
    "\n",
    "    # Initialized the DataFrame with cell barcodes\n",
    "    obs_df = pd.DataFrame({ 'barcodes' : bc })\n",
    "\n",
    "    # Get the list of available metadata columns\n",
    "    obs_columns = h5con['matrix']['observations'].keys()\n",
    "\n",
    "    # For each column\n",
    "    for col in obs_columns:\n",
    "        # Read the values\n",
    "        values = h5con['matrix']['observations'][col][:]\n",
    "        # Check for byte storage\n",
    "        if(isinstance(values[0], (bytes, bytearray))):\n",
    "            # Decode byte strings\n",
    "            values = [x.decode('UTF-8') for x in values]\n",
    "        # Add column to the DataFrame\n",
    "        obs_df[col] = values\n",
    "\n",
    "    obs_df = obs_df.set_index('barcodes', drop = False)\n",
    "    \n",
    "    return obs_df\n",
    "\n",
    "# define a function to construct anndata object from a h5 file\n",
    "def read_h5_anndata(h5_con):\n",
    "    #h5_con = h5py.File(h5_file, mode = 'r')\n",
    "    # extract the expression matrix\n",
    "    mat = read_mat(h5_con)\n",
    "    # extract gene names\n",
    "    genes = h5_con['matrix']['features']['name'][:]\n",
    "    genes = [x.decode('UTF-8') for x in genes]\n",
    "    # extract metadata\n",
    "    obs_df = read_obs(h5_con)\n",
    "    # construct anndata\n",
    "    adata = anndata.AnnData(mat.T,\n",
    "                             obs = obs_df)\n",
    "    # make sure the gene names aligned\n",
    "    adata.var_names = genes\n",
    "\n",
    "    adata.var_names_make_unique()\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bbb3440-41ca-4942-850d-1ad053da164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adata(uuid):\n",
    "    # Load the file using HISE\n",
    "    res = hisepy.reader.read_files([uuid])\n",
    "\n",
    "    # If there's an error, read_files returns a list instead of a dictionary.\n",
    "    # We should raise and exception with the message when this happens.\n",
    "    if(isinstance(res, list)):\n",
    "        error_message = res[0]['message']\n",
    "        raise Exception('{u}: {e}'.format(u = uuid, e = error_message))\n",
    "    \n",
    "    # Read the file to adata\n",
    "    h5_con = res['values'][0]\n",
    "    adata = read_h5_anndata(h5_con)\n",
    "    \n",
    "    # Close the file now that we're done with it\n",
    "    h5_con.close()\n",
    "\n",
    "    return(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bcd92d2-af9e-4e6f-8f51-450796d40476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(adata, model, model_name, out_dir = \"output\"):\n",
    "    # Make output directories\n",
    "    model_dir = \"{d}/{m}\".format(d = out_dir, m = model_name)\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    sample_id = adata.obs['pbmc_sample_id'].unique()[0]\n",
    "    label_file = \"{d}/{s}_{m}_labels.csv\".format(d = model_dir, s = sample_id, m = model_name)\n",
    "\n",
    "    if os.path.exists(label_file):\n",
    "        print(\"{s}: {m} Previously computed; Skipping.\".format(s = sample_id, m = model_name))\n",
    "    else:\n",
    "        # Perform prediction\n",
    "        predictions = celltypist.annotate(\n",
    "            adata, \n",
    "            model = model, \n",
    "            majority_voting = True)\n",
    "    \n",
    "        # Write output\n",
    "        \n",
    "        prob_file = \"{d}/{s}_{m}_probability_mat.parquet\".format(d = model_dir, s = sample_id, m = model_name)\n",
    "        prob = predictions.probability_matrix\n",
    "        prob.to_parquet(prob_file)\n",
    "    \n",
    "        dec_file = \"{d}/{s}_{m}_decision_mat.parquet\".format(d = model_dir, s = sample_id, m = model_name)\n",
    "        predictions.decision_matrix.to_parquet(dec_file)\n",
    "        \n",
    "        labels = predictions.predicted_labels\n",
    "        labels = labels.rename({'predicted_labels': model_name}, axis = 1)\n",
    "        \n",
    "        prob_scores = []\n",
    "        for i in range(labels.shape[0]):\n",
    "            prob_scores.append(prob.loc[labels.index.to_list()[i],labels[model_name][i]])\n",
    "        labels['{m}_score'.format(m = model_name)] = prob_scores\n",
    "        labels.to_csv(label_file)\n",
    "    \n",
    "def process_data(file_uuid, sample_id):\n",
    "    out_dir = \"output\"\n",
    "    check_file = '{d}/{m}/{s}_{m}_labels.csv'.format(d = out_dir, m = 'AIFI_L3', s = sample_id)\n",
    "\n",
    "    if os.path.exists(check_file):\n",
    "        print('{s} Previously labeled; Skipping.'.format(s = sample_id))\n",
    "    else:\n",
    "        # Load cells from HISE .h5 files\n",
    "        adata = get_adata(file_uuid)\n",
    "        \n",
    "        # Normalize data\n",
    "        sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "        adata.obs.index = adata.obs['barcodes']\n",
    "        \n",
    "        # Predict cell types\n",
    "        for model_name,model_path in model_paths.items():\n",
    "            run_prediction(\n",
    "                adata,\n",
    "                model_path,\n",
    "                model_name,\n",
    "                out_dir\n",
    "            )\n",
    "        \n",
    "        del adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97c30414-e0ce-4746-9e3f-b52f2e6db98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_id(n = 3):\n",
    "    import periodictable\n",
    "    from random import randrange\n",
    "    rand_el = []\n",
    "    for i in range(n):\n",
    "        el = randrange(0,118)\n",
    "        rand_el.append(periodictable.elements[el].name)\n",
    "    rand_str = '-'.join(rand_el)\n",
    "    return rand_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd8976-9454-49e0-932f-e8a13dd8c768",
   "metadata": {},
   "source": [
    "## Obtain CellTypist Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49791ed9-fe42-44bd-bc04-e395ae5db538",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uuids = {\n",
    "    'AIFI_L3': '671d1e43-bd32-4fea-bdda-d19a0484e664',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "824be9f5-c36c-425b-b439-4f8419c3e043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading fileID: 671d1e43-bd32-4fea-bdda-d19a0484e664\n",
      "Files have been successfully downloaded!\n"
     ]
    }
   ],
   "source": [
    "model_paths = {}\n",
    "for name,uuid in model_uuids.items():\n",
    "    model_paths[name] = read_path_uuid(uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38abd1d6-df98-4e6a-beba-cc06c55e52cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AIFI_L3': '/home/jupyter/cache/671d1e43-bd32-4fea-bdda-d19a0484e664/ref_pbmc_clean_celltypist_model_AIFI_L3_2024-04-19.pkl'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a4a141-780e-4eab-b9c1-d7b5865dfabd",
   "metadata": {},
   "source": [
    "## Read sample metadata from HISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f55a5a2-a5e0-42ce-a34c-9add7cbef948",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_meta_file_uuid = 'd82c5c42-ae5f-4e67-956e-cd3b7bf88105'\n",
    "file_query = hisepy.reader.read_files(\n",
    "    [sample_meta_file_uuid]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21ce05a3-f4dc-4475-923a-f7049b833326",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = file_query['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58473e89-5816-4b27-a3db-4b0fc3dae1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(868, 33)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2aa9e-c8ff-45de-aff8-d025ee1dfef8",
   "metadata": {},
   "source": [
    "### Filter metadata for selected cohort and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6afd4c07-1c1e-488c-8663-6c25b39e08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = meta_data[meta_data['cohort.cohortGuid'] == cohort]\n",
    "meta_data = meta_data[meta_data['subject.biologicalSex'] == subject_sex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b98a13c-7d7e-4e64-a779-033419284ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238, 33)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79d0ec-ec3a-4644-a149-a3058255904f",
   "metadata": {},
   "source": [
    "## Apply across files\n",
    "\n",
    "Here, we'll use `concurrent.futures` to apply the function above to our files in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e28d2e7-a974-4721-8d86-ff79605aa155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_dir = 'output'\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4932e96-7872-40a8-8ef5-f3fd81367394",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_uuids = meta_data['file.id'].to_list()\n",
    "sample_ids = meta_data['pbmc_sample_id'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06510b9e-62f9-4fd9-812c-5f94603b3b29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PB00001-01 Previously labeled; Skipping.PB00003-01 Previously labeled; Skipping.PB00006-01 Previously labeled; Skipping.PB00007-01 Previously labeled; Skipping.PB00013-01 Previously labeled; Skipping.PB00019-01 Previously labeled; Skipping.PB00024-01 Previously labeled; Skipping.PB00027-05 Previously labeled; Skipping.PB00030-02 Previously labeled; Skipping.PB00031-05 Previously labeled; Skipping.PB00033-06 Previously labeled; Skipping.PB00037-01 Previously labeled; Skipping.PB00040-01 Previously labeled; Skipping.PB00032-05 Previously labeled; Skipping.\n",
      "PB00047-01 Previously labeled; Skipping.PB00035-01 Previously labeled; Skipping.PB00050-01 Previously labeled; Skipping.PB00046-01 Previously labeled; Skipping.\n",
      "PB00142-01 Previously labeled; Skipping.PB00144-01 Previously labeled; Skipping.PB00153-01 Previously labeled; Skipping.PB00154-01 Previously labeled; Skipping.PB00147-01 Previously labeled; Skipping.PB00152-01 Previously labeled; Skipping.PB00155-01 Previously labeled; Skipping.PB00162-01 Previously labeled; Skipping.PB00253-01 Previously labeled; Skipping.PB00166-01 Previously labeled; Skipping.PB00256-01 Previously labeled; Skipping.PB00259-01 Previously labeled; Skipping.PB00262-01 Previously labeled; Skipping.PB00263-03 Previously labeled; Skipping.PB00257-01 Previously labeled; Skipping.PB00271-03 Previously labeled; Skipping.PB00272-01 Previously labeled; Skipping.PB00282-01 Previously labeled; Skipping.PB00281-01 Previously labeled; Skipping.PB00276-03 Previously labeled; Skipping.PB00278-05 Previously labeled; Skipping.PB00286-05 Previously labeled; Skipping.PB00298-02 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB00325-01 Previously labeled; Skipping.\n",
      "PB00327-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB00332-01 Previously labeled; Skipping.PB00331-01 Previously labeled; Skipping.\n",
      "\n",
      "PB00338-01 Previously labeled; Skipping.PB00347-01 Previously labeled; Skipping.PB00345-01 Previously labeled; Skipping.\n",
      "PB00350-01 Previously labeled; Skipping.\n",
      "PB00355-01 Previously labeled; Skipping.PB00367-01 Previously labeled; Skipping.PB00365-01 Previously labeled; Skipping.PB00373-01 Previously labeled; Skipping.PB00372-01 Previously labeled; Skipping.PB00376-01 Previously labeled; Skipping.PB00385-01 Previously labeled; Skipping.PB00380-01 Previously labeled; Skipping.PB00392-01 Previously labeled; Skipping.PB00502-01 Previously labeled; Skipping.PB00525-01 Previously labeled; Skipping.\n",
      "\n",
      "PB00532-01 Previously labeled; Skipping.PB00529-01 Previously labeled; Skipping.\n",
      "PB00528-01 Previously labeled; Skipping.PB00546-01 Previously labeled; Skipping.PB00544-01 Previously labeled; Skipping.PB00555-01 Previously labeled; Skipping.PB00547-01 Previously labeled; Skipping.PB00576-01 Previously labeled; Skipping.PB00575-01 Previously labeled; Skipping.PB00586-01 Previously labeled; Skipping.PB00587-01 Previously labeled; Skipping.\n",
      "PB00585-01 Previously labeled; Skipping.\n",
      "\n",
      "PB00589-01 Previously labeled; Skipping.PB00591-01 Previously labeled; Skipping.\n",
      "\n",
      "PB00594-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB00595-01 Previously labeled; Skipping.PB00601-01 Previously labeled; Skipping.\n",
      "\n",
      "PB00610-02 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB00612-01 Previously labeled; Skipping.PB00617-01 Previously labeled; Skipping.PB00616-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB00622-01 Previously labeled; Skipping.PB00621-01 Previously labeled; Skipping.PB00628-01 Previously labeled; Skipping.PB00630-01 Previously labeled; Skipping.PB00631-01 Previously labeled; Skipping.PB00633-01 Previously labeled; Skipping.PB00634-01 Previously labeled; Skipping.PB00635-01 Previously labeled; Skipping.PB00643-01 Previously labeled; Skipping.\n",
      "\n",
      "PB00645-01 Previously labeled; Skipping.PB00650-01 Previously labeled; Skipping.PB00646-01 Previously labeled; Skipping.\n",
      "PB01411-01 Previously labeled; Skipping.PB01425-01 Previously labeled; Skipping.PB01430-01 Previously labeled; Skipping.PB01435-01 Previously labeled; Skipping.PB01434-01 Previously labeled; Skipping.PB01440-01 Previously labeled; Skipping.PB01450-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB01457-01 Previously labeled; Skipping.\n",
      "\n",
      "PB01514-01 Previously labeled; Skipping.PB01454-01 Previously labeled; Skipping.\n",
      "\n",
      "PB01520-01 Previously labeled; Skipping.PB01524-01 Previously labeled; Skipping.PB01459-01 Previously labeled; Skipping.PB01519-01 Previously labeled; Skipping.\n",
      "\n",
      "PB01531-01 Previously labeled; Skipping.PB01525-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB01548-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB01552-01 Previously labeled; Skipping.PB01559-01 Previously labeled; Skipping.PB01565-01 Previously labeled; Skipping.PB01573-01 Previously labeled; Skipping.PB01574-01 Previously labeled; Skipping.\n",
      "PB01583-01 Previously labeled; Skipping.PB01587-01 Previously labeled; Skipping.PB01602-01 Previously labeled; Skipping.PB01596-01 Previously labeled; Skipping.\n",
      "\n",
      "PB01603-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB01972-01 Previously labeled; Skipping.PB01975-01 Previously labeled; Skipping.PB02238-01 Previously labeled; Skipping.PB02233-01 Previously labeled; Skipping.\n",
      "PB02242-02 Previously labeled; Skipping.PB02247-01 Previously labeled; Skipping.PB02244-01 Previously labeled; Skipping.PB02253-01 Previously labeled; Skipping.PB02263-02 Previously labeled; Skipping.PB02257-01 Previously labeled; Skipping.PB02259-01 Previously labeled; Skipping.PB02266-01 Previously labeled; Skipping.PB02264-01 Previously labeled; Skipping.PB02267-01 Previously labeled; Skipping.PB02274-01 Previously labeled; Skipping.PB02271-01 Previously labeled; Skipping.PB02270-01 Previously labeled; Skipping.PB02276-02 Previously labeled; Skipping.PB02311-001 Previously labeled; Skipping.PB02312-001 Previously labeled; Skipping.PB02333-001 Previously labeled; Skipping.PB02275-01 Previously labeled; Skipping.PB02352-001 Previously labeled; Skipping.PB02336-001 Previously labeled; Skipping.PB02343-001 Previously labeled; Skipping.PB02338-001 Previously labeled; Skipping.PB02446-001 Previously labeled; Skipping.PB02444-001 Previously labeled; Skipping.\n",
      "PB02248-01 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "PB02449-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB02462-001 Previously labeled; Skipping.PB02466-001 Previously labeled; Skipping.PB02465-001 Previously labeled; Skipping.\n",
      "PB02470-001 Previously labeled; Skipping.PB02468-001 Previously labeled; Skipping.PB02475-001 Previously labeled; Skipping.PB02471-001 Previously labeled; Skipping.PB02476-001 Previously labeled; Skipping.PB02480-001 Previously labeled; Skipping.PB02483-001 Previously labeled; Skipping.PB02486-001 Previously labeled; Skipping.PB02490-001 Previously labeled; Skipping.PB02494-001 Previously labeled; Skipping.PB02496-001 Previously labeled; Skipping.PB02520-001 Previously labeled; Skipping.PB02519-001 Previously labeled; Skipping.PB02529-001 Previously labeled; Skipping.PB02550-001 Previously labeled; Skipping.PB02552-001 Previously labeled; Skipping.PB02553-002 Previously labeled; Skipping.PB02559-002 Previously labeled; Skipping.PB02556-002 Previously labeled; Skipping.\n",
      "PB02557-002 Previously labeled; Skipping.PB02561-001 Previously labeled; Skipping.\n",
      "\n",
      "PB02564-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB02571-001 Previously labeled; Skipping.PB02577-001 Previously labeled; Skipping.PB02572-001 Previously labeled; Skipping.PB02578-001 Previously labeled; Skipping.PB02581-001 Previously labeled; Skipping.PB02589-001 Previously labeled; Skipping.PB02591-001 Previously labeled; Skipping.PB02590-001 Previously labeled; Skipping.PB03062-002 Previously labeled; Skipping.PB03068-002 Previously labeled; Skipping.PB02592-002 Previously labeled; Skipping.PB03074-001 Previously labeled; Skipping.PB03083-001 Previously labeled; Skipping.PB03091-001 Previously labeled; Skipping.PB03096-001 Previously labeled; Skipping.PB03101-001 Previously labeled; Skipping.PB03107-001 Previously labeled; Skipping.PB03105-001 Previously labeled; Skipping.PB03904-001 Previously labeled; Skipping.PB03915-001 Previously labeled; Skipping.PB03920-001 Previously labeled; Skipping.PB03923-001 Previously labeled; Skipping.PB03922-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PB03925-001 Previously labeled; Skipping.PB03928-001 Previously labeled; Skipping.PB03926-001 Previously labeled; Skipping.\n",
      "\n",
      "\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔬 Input data has 20227 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 18124 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🔬 Input data has 16534 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🔬 Input data has 20633 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🔬 Input data has 19186 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 16681 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 14122 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🔬 Input data has 13829 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🔬 Input data has 13768 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🖋️ Predicting labels\n",
      "🖋️ Predicting labels\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "✅ Prediction done!\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🔬 Input data has 20481 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 16151 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🔬 Input data has 21192 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 14642 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 18547 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 16748 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to read from file\n",
      "Error: Unable to read from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔬 Input data has 20618 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 17918 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🖋️ Predicting labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to read from file\n",
      "Error: Unable to read from file\n",
      "Error: Unable to read from file\n",
      "Error: Unable to read from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🔬 Input data has 20068 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 23491 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 19768 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 21351 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 19294 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 18359 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 20854 cells and 33538 genes\n",
      "🔬 Input data has 17971 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 19648 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 18279 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🔬 Input data has 19140 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🔬 Input data has 20346 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🔬 Input data has 17602 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🖋️ Predicting labels\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🧬 2504 features used for prediction\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "⚖️ Scaling input data\n",
      "🔬 Input data has 20476 cells and 33538 genes\n",
      "🔗 Matching reference genes in the model\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🖋️ Predicting labels\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🖋️ Predicting labels\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🖋️ Predicting labels\n",
      "🖋️ Predicting labels\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "✅ Prediction done!\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🖋️ Predicting labels\n",
      "🖋️ Predicting labels\n",
      "🧬 2504 features used for prediction\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "⚖️ Scaling input data\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🖋️ Predicting labels\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🧬 2504 features used for prediction\n",
      "⚖️ Scaling input data\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n",
      "🖋️ Predicting labels\n",
      "✅ Prediction done!\n",
      "👀 Can not detect a neighborhood graph, will construct one before the over-clustering\n"
     ]
    }
   ],
   "source": [
    "# Process each subset in parallel\n",
    "pool_executor = concurrent.futures.ProcessPoolExecutor(max_workers = 60)\n",
    "with pool_executor as executor:\n",
    "    \n",
    "    futures = []\n",
    "    for i in range(len(file_uuids)):\n",
    "        file_uuid = file_uuids[i]\n",
    "        sample_id = sample_ids[i]\n",
    "        futures.append(executor.submit(process_data, file_uuid, sample_id))\n",
    "\n",
    "    # Check for errors when parallel processes return results\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            print(future.result())\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74db89fd-2f92-4148-9a6d-f115c1efe52e",
   "metadata": {},
   "source": [
    "## Assemble results\n",
    "\n",
    "For each model, we'll assemble the results as a .csv file that we can utilize later for subclustering and analysis of major cell classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c2832-1f92-4830-95c1-b87d44c6e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(model_paths.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942b3d87-2fbb-461d-8f04-89c13118d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_files = []\n",
    "for model in models:\n",
    "    model_path = 'output/{m}'.format(m = model)\n",
    "    model_path_files = os.listdir(model_path)\n",
    "    model_files = []\n",
    "    for model_path_file in model_path_files:\n",
    "        if 'labels' in model_path_file:\n",
    "            model_files.append(model_path_file)\n",
    "    print(len(model_files))\n",
    "    \n",
    "    model_list = []\n",
    "    for model_file in model_files:\n",
    "        df = pd.read_csv('output/{m}/{f}'.format(m = model, f = model_file))\n",
    "        model_list.append(df)\n",
    "    model_df = pd.concat(model_list)\n",
    "\n",
    "    out_csv = 'output/diha_celltypist_{c}_{s}_{m}_{d}.csv'.format(\n",
    "        c = cohort, s = subject_sex, m = model, d = date.today())\n",
    "    out_files.append(out_csv)\n",
    "    \n",
    "    model_df.to_csv(out_csv)\n",
    "\n",
    "    out_parquet = 'output/diha_celltypist_{c}_{s}_{m}_{d}.parquet'.format(\n",
    "        c = cohort, s = subject_sex, m = model, d = date.today())\n",
    "    out_files.append(out_parquet)\n",
    "    \n",
    "    model_df.to_parquet(out_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd746b1-8066-44a5-8613-f401e6fce07e",
   "metadata": {},
   "source": [
    "## Upload assembled data to HISE\n",
    "\n",
    "Finally, we'll use `hisepy.upload.upload_files()` to send a copy of our output to HISE to use for downstream analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66e3e599-872c-4677-8bee-08d04003958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_space_uuid = 'de025812-5e73-4b3c-9c3b-6d0eac412f2a'\n",
    "title = 'DIHA PBMC CellTypist L3 {c} {s} {d}'.format(\n",
    "    c = cohort, s = subject_sex, d = date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afa5544b-f81d-41bc-8fb6-a976b075f7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actinium-cobalt-dubnium'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_id = element_id()\n",
    "search_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bffe38f-4223-4919-b46a-a4b8536dde06",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_files = list(model_uuids.values()) + [sample_meta_file_uuid] + meta_data['file.id'].to_list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce9877bf-cc8a-4b92-8b9e-ba6f4964f79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['671d1e43-bd32-4fea-bdda-d19a0484e664',\n",
       " 'd82c5c42-ae5f-4e67-956e-cd3b7bf88105',\n",
       " 'fec489f9-9a74-4635-aa91-d2bf09d1faec',\n",
       " '40efd03a-cb2f-4677-af42-a056cbfe5a17',\n",
       " 'ea8d98e9-e99e-4dc6-9e78-9866e0deac68',\n",
       " '1faf2b5f-66e4-4787-8a8b-487621fc4c08',\n",
       " 'cda87fcc-a50e-4c0f-b26c-482a6a88ef41',\n",
       " '7a99c4c8-5438-430a-a37a-5b5f4052c064',\n",
       " 'cd86a3b7-4955-4d76-9b2c-f076024a04eb',\n",
       " '4980030b-919e-4ca6-87fb-29b9163d393c']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_files[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "544fb074-02d5-4ca4-968a-780710e5f48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/diha_celltypist_BR1_Female_AIFI_L3_2024-04-19.csv',\n",
       " 'output/diha_celltypist_BR1_Female_AIFI_L3_2024-04-19.parquet']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80a524b6-7c4c-48c1-8b8c-d05494737bb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The following file Ids were not downloaded in this IDE. You cannot reference a file in a result without downloading it first. ['cda87fcc-a50e-4c0f-b26c-482a6a88ef41', '7a99c4c8-5438-430a-a37a-5b5f4052c064', '9114124c-af57-47a7-bde3-ade4a150ac82', '9d5d8b77-6fb9-4f6c-8f0f-a24d87968962', '07528ecf-0d7d-4935-9244-b263883e69ca', '5b74b13b-d951-4d55-88d2-0f3b5f22842a', 'dffa2241-a366-44ff-8f0a-894dd7cbbe6c', '30faa01b-7e0a-41b5-a948-025aac545d4c', '97da56b5-604c-4a29-9dff-c837c87dc885', '6cc1cc8c-f32a-4f95-bc18-c7031aeff2e9', '57f4ee83-5d68-499f-9dc2-03308c37a840', 'f7639a2a-f833-4aa6-b44e-f8f82c1be5ec', '1eefbf6f-2f0d-4371-bad4-053989fe89a6', '8f355043-7aa5-4e33-8c6e-187905d32206', 'ad1e2a91-aa57-40c6-9068-09ff5adb6696', '34647c08-376a-44b9-a4da-149f218fae76', 'f6b968f9-44de-4798-9d3f-54150330c158', 'ed315c29-e352-4930-a682-1abed3194824', '0c6205b4-3a32-4143-b3c6-39b520eb3e26', '89659479-4bcf-4104-a9c3-52387b2a7c9b', 'f09dd579-1f4d-42a3-98b1-897ec41cab30', '0d91b339-9bf1-4a9b-827a-dc829b91ede8', 'cc4a7cfa-823a-4c8c-84e3-d8d0e95b3dab', '1bb95779-064e-4aff-8ae4-9f53ac53198f', '50e0228b-9d67-45a2-a5ee-a73ee6ab2a95', '96a931f2-7721-451b-9f8d-b2eaf3b5b029', '8dd5c8ca-5ff3-4bfd-aba7-dec02f78b750', '8147de2b-9fef-4d52-adce-08e31cabcbbe', '83ad0b29-37c2-4644-8d03-9c0e13a99515', '5efa981d-24d2-4dda-a101-1dbccbeb7991', '57a879d5-6423-469d-afdd-0e5d710f2735', 'df2a9523-a8ce-4b88-bab3-1d5af55252ef', 'de2f315a-7c23-4f06-b3be-914ad09a3cdc', 'b98f0e67-482b-4eaa-b795-bc8977084d14', 'eb88af98-2efc-4392-a653-db289f2bb115', '144ea1b3-d5b4-4639-bd9f-53415c90dac2', '5f2d6289-2eae-4849-b68c-48aa631eb279', 'd7660e92-a7f0-44e1-8d3e-4c571ea576f0', '04ad3351-8e26-44ad-8a0b-94158a66da79', 'f0ca9d87-c4fa-49f9-b390-2eb0e085cfdc', '1b323268-7ea5-49de-a10e-d085b8c5ca69', '9a22eebb-65dd-4886-808c-a16efe0b367d', 'ea52d9d6-c1c8-42f1-be3b-84e522ff6bd1', '6ec1b183-72f0-44c1-89fe-00987af02143', 'b095fb97-abdc-4bc2-9048-371e2b178f74', '14bc0bab-e76f-4939-b237-a8885159444e', '3741f323-836b-4418-8d76-109461976931', 'c102ab88-a8ac-476f-8506-37b6500760f2', '961cd967-ce23-49a1-af6e-532c873ba3f3', '54dd3fae-78dc-4821-a6ba-5dab647f95a9', '033df296-b45d-427d-93be-4efdf182943b', '372bdfa6-eea8-4e26-b967-8b40895ec5fa', 'e51a7298-24f6-4955-9343-dbe339dcbda3', 'e0537a81-d569-4dcb-bf7e-9a7717985b11']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhisepy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mout_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy_space_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstudy_space_uuid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_file_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43min_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msearch_id\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hisepy-0.3.0-py3.10.egg/hisepy/upload.py:154\u001b[0m, in \u001b[0;36mupload_files\u001b[0;34m(files, study_space_id, project, title, input_file_ids, input_sample_ids, file_types, store, destination, do_prompt)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cu\u001b[38;5;241m.\u001b[39mstring_contains_whitespaces(destination):\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestination param, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, contains whitespaces. Please rename and remove any whitespaces\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(destination))\n\u001b[0;32m--> 154\u001b[0m \u001b[43mcu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_upload_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_sample_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m validate_upload_data(files, study_space_id, project, title, input_file_ids)\n\u001b[1;32m    156\u001b[0m uploads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hisepy-0.3.0-py3.10.egg/hisepy/common_utils.py:190\u001b[0m, in \u001b[0;36mvalidate_upload_input_ids\u001b[0;34m(input_file_ids, input_sample_ids)\u001b[0m\n\u001b[1;32m    187\u001b[0m         invalid_sample_ids \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [s]\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(invalid_file_ids) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following file Ids were not downloaded in this IDE. You cannot reference a file in a result without downloading it first. \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(invalid_file_ids))\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(invalid_sample_ids) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following sample Ids were not downloaded in this IDE. You cannot refernce a file in a result without downloading it first. \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(invalid_sample_ids))\n",
      "\u001b[0;31mAssertionError\u001b[0m: The following file Ids were not downloaded in this IDE. You cannot reference a file in a result without downloading it first. ['cda87fcc-a50e-4c0f-b26c-482a6a88ef41', '7a99c4c8-5438-430a-a37a-5b5f4052c064', '9114124c-af57-47a7-bde3-ade4a150ac82', '9d5d8b77-6fb9-4f6c-8f0f-a24d87968962', '07528ecf-0d7d-4935-9244-b263883e69ca', '5b74b13b-d951-4d55-88d2-0f3b5f22842a', 'dffa2241-a366-44ff-8f0a-894dd7cbbe6c', '30faa01b-7e0a-41b5-a948-025aac545d4c', '97da56b5-604c-4a29-9dff-c837c87dc885', '6cc1cc8c-f32a-4f95-bc18-c7031aeff2e9', '57f4ee83-5d68-499f-9dc2-03308c37a840', 'f7639a2a-f833-4aa6-b44e-f8f82c1be5ec', '1eefbf6f-2f0d-4371-bad4-053989fe89a6', '8f355043-7aa5-4e33-8c6e-187905d32206', 'ad1e2a91-aa57-40c6-9068-09ff5adb6696', '34647c08-376a-44b9-a4da-149f218fae76', 'f6b968f9-44de-4798-9d3f-54150330c158', 'ed315c29-e352-4930-a682-1abed3194824', '0c6205b4-3a32-4143-b3c6-39b520eb3e26', '89659479-4bcf-4104-a9c3-52387b2a7c9b', 'f09dd579-1f4d-42a3-98b1-897ec41cab30', '0d91b339-9bf1-4a9b-827a-dc829b91ede8', 'cc4a7cfa-823a-4c8c-84e3-d8d0e95b3dab', '1bb95779-064e-4aff-8ae4-9f53ac53198f', '50e0228b-9d67-45a2-a5ee-a73ee6ab2a95', '96a931f2-7721-451b-9f8d-b2eaf3b5b029', '8dd5c8ca-5ff3-4bfd-aba7-dec02f78b750', '8147de2b-9fef-4d52-adce-08e31cabcbbe', '83ad0b29-37c2-4644-8d03-9c0e13a99515', '5efa981d-24d2-4dda-a101-1dbccbeb7991', '57a879d5-6423-469d-afdd-0e5d710f2735', 'df2a9523-a8ce-4b88-bab3-1d5af55252ef', 'de2f315a-7c23-4f06-b3be-914ad09a3cdc', 'b98f0e67-482b-4eaa-b795-bc8977084d14', 'eb88af98-2efc-4392-a653-db289f2bb115', '144ea1b3-d5b4-4639-bd9f-53415c90dac2', '5f2d6289-2eae-4849-b68c-48aa631eb279', 'd7660e92-a7f0-44e1-8d3e-4c571ea576f0', '04ad3351-8e26-44ad-8a0b-94158a66da79', 'f0ca9d87-c4fa-49f9-b390-2eb0e085cfdc', '1b323268-7ea5-49de-a10e-d085b8c5ca69', '9a22eebb-65dd-4886-808c-a16efe0b367d', 'ea52d9d6-c1c8-42f1-be3b-84e522ff6bd1', '6ec1b183-72f0-44c1-89fe-00987af02143', 'b095fb97-abdc-4bc2-9048-371e2b178f74', '14bc0bab-e76f-4939-b237-a8885159444e', '3741f323-836b-4418-8d76-109461976931', 'c102ab88-a8ac-476f-8506-37b6500760f2', '961cd967-ce23-49a1-af6e-532c873ba3f3', '54dd3fae-78dc-4821-a6ba-5dab647f95a9', '033df296-b45d-427d-93be-4efdf182943b', '372bdfa6-eea8-4e26-b967-8b40895ec5fa', 'e51a7298-24f6-4955-9343-dbe339dcbda3', 'e0537a81-d569-4dcb-bf7e-9a7717985b11']"
     ]
    }
   ],
   "source": [
    "hisepy.upload.upload_files(\n",
    "    files = out_files,\n",
    "    study_space_id = study_space_uuid,\n",
    "    title = title,\n",
    "    input_file_ids = in_files,\n",
    "    destination = search_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df57b01-3914-48ae-9cc2-f03faec7473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import session_info\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a739960e-997b-46f8-9cd4-90bdeb3d672c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
